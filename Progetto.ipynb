{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979eba2c-cb7e-4636-b3f4-5712f2c90ed3",
   "metadata": {},
   "source": [
    "## Import e Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d138f55a-471a-474f-ab6b-87d4a68e6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import HeNormal\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignora un FutureWarning di Pandas\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "422e336c-9061-4ee0-bd08-3615548ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_result_to_csv(model_name, accuracy, file_name='results.csv'):\n",
    "    # Check if the file exists\n",
    "    file_exists = os.path.isfile(file_name)\n",
    "    \n",
    "    # If the file exists, read it into a DataFrame\n",
    "    if file_exists:\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        # If the file does not exist, create an empty DataFrame\n",
    "        df = pd.DataFrame(columns=['Model', 'Accuracy'])\n",
    "    \n",
    "    # Create a new DataFrame with the new result\n",
    "    new_data = pd.DataFrame([[model_name, accuracy]], columns=['Model', 'Accuracy'])\n",
    "    \n",
    "    # Concatenate the existing DataFrame with the new data\n",
    "    df = pd.concat([df, new_data], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c425ed5-feaa-4445-941c-8e54e6bc00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_accuracy_from_csv(model_name, csv_file_name='results.csv'):\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(csv_file_name)\n",
    "\n",
    "        # Find the accuracy corresponding to the model name\n",
    "        accuracy = df.loc[df['Model'] == model_name, 'Accuracy'].values[0]\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{csv_file_name}' not found.\")\n",
    "        return None\n",
    "    except IndexError:\n",
    "        print(f\"Model '{model_name}' not found in file '{csv_file_name}'.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8be11a6-3c5e-441f-afa0-e5d60e122164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di GPU disponibili: 1\n",
      "Nome GPU: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Controlla le GPU disponibili\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"Numero di GPU disponibili: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Nome GPU: {gpu.name}\")\n",
    "else:\n",
    "    print(\"Nessuna GPU disponibile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0813e-c0cb-4069-9ce5-ccb31e81bb43",
   "metadata": {},
   "source": [
    "## Import e Pre-processamento Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadfb557-a422-4fd4-819f-2f998328a228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di immagini totale =  5232\n",
      "Numero di immagini nel testing set =  524\n"
     ]
    }
   ],
   "source": [
    "x = np.load(r\"C:\\Users\\filip\\Desktop\\Filippo\\Uni\\Magistrale\\ML\\pneumonia_images.npy\")\n",
    "y = np.load(r\"C:\\Users\\filip\\Desktop\\Filippo\\Uni\\Magistrale\\ML\\pneumonia_labels.npy\")\n",
    "\n",
    "#creazione dei vari dataset\n",
    "seed = 1999\n",
    "#separazione train e test set\n",
    "train_images, test_images = train_test_split(x, test_size=0.1, random_state=seed)\n",
    "train_labels, test_labels = train_test_split(y, test_size=0.1, random_state=seed)\n",
    "\n",
    "print(\"Numero di immagini totale = \", len(x))\n",
    "print(\"Numero di immagini nel testing set = \", len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa201aa-1e93-42b0-b3ba-c864905a274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di immagini nel training set =  3765\n",
      "Numero di immagini nel validation set =  943\n"
     ]
    }
   ],
   "source": [
    "# creazione validation set e train set con split 80/20 a favore del training set\n",
    "x_valid_images = train_images[3765:]/255. # normalizzazione\n",
    "x_valid_labels = train_labels[3765:]\n",
    "x_train_images = train_images[:3765]/255. # normalizzazione\n",
    "x_train_labels = train_labels[:3765]\n",
    "\n",
    "#normalizzazione test set\n",
    "test_images = test_images/255.\n",
    "\n",
    "print(\"Numero di immagini nel training set = \", len(x_train_images))\n",
    "print(\"Numero di immagini nel validation set = \", len(x_valid_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b4b366-e10a-4964-ad4f-cf96c9c8d98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(0, 9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_train_images[i], cmap='gray')\n",
    "    label = 'Normal' if  x_train_labels[i] == 0 else 'Pneumonia'\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fd66c23-19fd-4528-b6e3-d49b98d10016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Conta le occorrenze di ciascuna etichetta\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "labels_counts = dict(zip(unique, counts))\n",
    "\n",
    "# Crea il grafico a colonne\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels_counts.keys(), labels_counts.values(), color=['darkgreen', 'darkred'])\n",
    "\n",
    "# Imposta i titoli e le etichette\n",
    "ax.set_xlabel('Classe')\n",
    "ax.set_ylabel('Numero di Esempi')\n",
    "ax.set_title('Distribuzione Esempi nel Dataset')\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_xticklabels(['Normal', 'Pneumonia'])\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058df044-f98e-4d08-81c9-d097d99cb057",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0e2c430-c005-45e3-ac11-f594acedb856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "Best parameters found:  {'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Validation set accuracy:  0.95864262990456\n",
      "Test set accuracy:  0.9599236641221374\n"
     ]
    }
   ],
   "source": [
    "x_train_flat = x_train_images.reshape(x_train_images.shape[0], -1)\n",
    "x_valid_flat = x_valid_images.reshape(x_valid_images.shape[0], -1)\n",
    "test_flat = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "# Creazione del modello di Random Forest\n",
    "rf = RandomForestClassifier(random_state=42) # seed per rendere l'esecuzione ripetibile\n",
    "\n",
    "# Definizione della griglia degli iperparametri\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [None, 30],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Creazione della grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Addestramento della grid search\n",
    "grid_search.fit(x_train_flat, x_train_labels.ravel()) # circa 10 minuti per la grid search\n",
    "\n",
    "# Migliori iperparametri trovati\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Valutazione del modello sul validation set\n",
    "best_rf = grid_search.best_estimator_\n",
    "valid_predictions = best_rf.predict(x_valid_flat)\n",
    "accuracy = accuracy_score(x_valid_labels.ravel(), valid_predictions)\n",
    "print(\"Validation set accuracy: \", accuracy)\n",
    "\n",
    "# Valutazione finale sul test set\n",
    "test_predictions = best_rf.predict(test_flat)\n",
    "test_accuracy_random_forest = accuracy_score(test_labels.ravel(), test_predictions)\n",
    "print(\"Test set accuracy: \", test_accuracy_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "906864bd-49a2-4386-a83c-b1f54dec4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result_to_csv('Random Forest', test_accuracy_random_forest) # aggiunta entry per RF nel .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0b09eb1-1491-4dd7-9efd-69a8b9d01d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       132\n",
      "           1       0.97      0.97      0.97       392\n",
      "\n",
      "    accuracy                           0.96       524\n",
      "   macro avg       0.95      0.95      0.95       524\n",
      "weighted avg       0.96      0.96      0.96       524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_test = classification_report(test_labels.ravel(), test_predictions)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006cbe55-cd72-4fa7-b660-c37d1358205b",
   "metadata": {},
   "source": [
    "## Regressione Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d280747-99b7-4d70-a29b-78f0a5423643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del modello di Logistic Regression\n",
    "# Utilizzando Pipeline per standardizzare i dati e applicare la regressione logistica\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardizzazione dei dati\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=100))\n",
    "])\n",
    "\n",
    "# Definizione della griglia degli iperparametri per la regressione logistica\n",
    "param_grid_logreg = [\n",
    "    {'logreg__C': [0.01, 0.1, 1, 10, 100], # termine per la regolarizzazione. Pi  piccolo pi  forte \n",
    "     'logreg__solver': ['liblinear'],\n",
    "     'logreg__penalty': ['l1', 'l2']},\n",
    "    {'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "     'logreg__solver': ['saga'],\n",
    "     'logreg__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "     'logreg__l1_ratio': [0.5]},  # l1_ratio  richiesto per elasticnet\n",
    "    {'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "     'logreg__solver': ['newton-cg', 'lbfgs', 'sag', 'newton-cholesky'],\n",
    "     'logreg__penalty': ['l2']}\n",
    "]\n",
    "# Creazione della grid search\n",
    "grid_search_logreg = GridSearchCV(estimator=pipe, param_grid=param_grid_logreg, cv=3, n_jobs=-1, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1fd7b05-9888-4d1a-87ee-59eb45c07451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n",
      "Best parameters found for Logistic Regression:  {'logreg__C': 0.1, 'logreg__penalty': 'l2', 'logreg__solver': 'sag'}\n",
      "Validation set accuracy for Logistic Regression:  0.9618239660657476\n",
      "Test set accuracy for Logistic Regression:  0.9675572519083969\n"
     ]
    }
   ],
   "source": [
    "# Addestramento della grid search\n",
    "grid_search_logreg.fit(x_train_flat, x_train_labels.ravel())\n",
    "\n",
    "# Migliori iperparametri trovati\n",
    "print(\"Best parameters found for Logistic Regression: \", grid_search_logreg.best_params_)\n",
    "\n",
    "# Valutazione del modello sul validation set\n",
    "best_logreg = grid_search_logreg.best_estimator_\n",
    "valid_predictions_logreg = best_logreg.predict(x_valid_flat)\n",
    "accuracy_logreg = accuracy_score(x_valid_labels.ravel(), valid_predictions_logreg)\n",
    "print(\"Validation set accuracy for Logistic Regression: \", accuracy_logreg)\n",
    "\n",
    "# Valutazione finale sul test set\n",
    "test_predictions_logreg = best_logreg.predict(test_flat)\n",
    "test_accuracy_logreg = accuracy_score(test_labels.ravel(), test_predictions_logreg)\n",
    "print(\"Test set accuracy for Logistic Regression: \", test_accuracy_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e3a4585-85a8-4301-a53f-b8de0ffd97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_result_to_csv('Logistic Regression', test_accuracy_logreg) # aggiunta entry per RF nel .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d6c6935-aada-4e82-ac29-3eb904eb3972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90       132\n",
      "           1       0.96      0.97      0.97       392\n",
      "\n",
      "    accuracy                           0.95       524\n",
      "   macro avg       0.93      0.93      0.93       524\n",
      "weighted avg       0.95      0.95      0.95       524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_test_logreg = classification_report(test_labels.ravel(), test_predictions_logreg)\n",
    "print(report_test_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988d450-7a05-4cd4-bbe2-f4088feeb657",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23d98286-6031-4c04-a970-27d5fa0be744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "118/118 [==============================] - 2s 9ms/step - loss: 0.3157 - accuracy: 0.8598 - val_loss: 0.2087 - val_accuracy: 0.9088\n",
      "Epoch 2/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.1893 - accuracy: 0.9227 - val_loss: 0.1783 - val_accuracy: 0.9247\n",
      "Epoch 3/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1620 - accuracy: 0.9331 - val_loss: 0.1294 - val_accuracy: 0.9533\n",
      "Epoch 4/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.1519 - accuracy: 0.9378 - val_loss: 0.1269 - val_accuracy: 0.9470\n",
      "Epoch 5/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1511 - accuracy: 0.9413 - val_loss: 0.1568 - val_accuracy: 0.9343\n",
      "Epoch 6/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.1406 - accuracy: 0.9450 - val_loss: 0.2031 - val_accuracy: 0.9141\n",
      "Epoch 7/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1228 - accuracy: 0.9556 - val_loss: 0.1010 - val_accuracy: 0.9586\n",
      "Epoch 8/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.1188 - accuracy: 0.9538 - val_loss: 0.0983 - val_accuracy: 0.9629\n",
      "Epoch 9/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1122 - accuracy: 0.9580 - val_loss: 0.1148 - val_accuracy: 0.9597\n",
      "Epoch 10/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0985 - accuracy: 0.9625 - val_loss: 0.1467 - val_accuracy: 0.9417\n",
      "Epoch 11/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.1068 - accuracy: 0.9543 - val_loss: 0.0961 - val_accuracy: 0.9661\n",
      "Epoch 12/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.1004 - accuracy: 0.9623 - val_loss: 0.1078 - val_accuracy: 0.9576\n",
      "Epoch 13/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0901 - accuracy: 0.9676 - val_loss: 0.1109 - val_accuracy: 0.9597\n",
      "Epoch 14/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0852 - accuracy: 0.9673 - val_loss: 0.0848 - val_accuracy: 0.9682\n",
      "Epoch 15/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0760 - accuracy: 0.9713 - val_loss: 0.0847 - val_accuracy: 0.9661\n",
      "Epoch 16/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0824 - accuracy: 0.9689 - val_loss: 0.0818 - val_accuracy: 0.9682\n",
      "Epoch 17/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.0722 - accuracy: 0.9726 - val_loss: 0.0847 - val_accuracy: 0.9703\n",
      "Epoch 18/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0740 - accuracy: 0.9729 - val_loss: 0.0834 - val_accuracy: 0.9703\n",
      "Epoch 19/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0673 - accuracy: 0.9766 - val_loss: 0.0914 - val_accuracy: 0.9576\n",
      "Epoch 20/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0691 - accuracy: 0.9740 - val_loss: 0.0817 - val_accuracy: 0.9682\n",
      "Epoch 21/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.0680 - accuracy: 0.9756 - val_loss: 0.0946 - val_accuracy: 0.9650\n",
      "Epoch 22/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0670 - accuracy: 0.9748 - val_loss: 0.0835 - val_accuracy: 0.9714\n",
      "Epoch 23/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.0599 - accuracy: 0.9761 - val_loss: 0.0964 - val_accuracy: 0.9608\n",
      "Epoch 24/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0542 - accuracy: 0.9801 - val_loss: 0.0812 - val_accuracy: 0.9714\n",
      "Epoch 25/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0566 - accuracy: 0.9785 - val_loss: 0.0917 - val_accuracy: 0.9671\n",
      "Epoch 26/300\n",
      "118/118 [==============================] - 1s 5ms/step - loss: 0.0520 - accuracy: 0.9814 - val_loss: 0.0825 - val_accuracy: 0.9714\n",
      "Epoch 27/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.0512 - accuracy: 0.9822 - val_loss: 0.0998 - val_accuracy: 0.9618\n",
      "Epoch 28/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0500 - accuracy: 0.9817 - val_loss: 0.0955 - val_accuracy: 0.9661\n",
      "Epoch 29/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.0409 - accuracy: 0.9859 - val_loss: 0.0883 - val_accuracy: 0.9735\n",
      "Epoch 30/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0412 - accuracy: 0.9870 - val_loss: 0.0962 - val_accuracy: 0.9671\n",
      "Epoch 31/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0449 - accuracy: 0.9835 - val_loss: 0.1187 - val_accuracy: 0.9650\n",
      "Epoch 32/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0384 - accuracy: 0.9865 - val_loss: 0.0951 - val_accuracy: 0.9745\n",
      "Epoch 33/300\n",
      "118/118 [==============================] - 1s 6ms/step - loss: 0.0313 - accuracy: 0.9886 - val_loss: 0.1309 - val_accuracy: 0.9586\n",
      "Epoch 34/300\n",
      "118/118 [==============================] - 1s 7ms/step - loss: 0.0294 - accuracy: 0.9904 - val_loss: 0.1224 - val_accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "x_reshaped_train = x_train_images.reshape(-1,28,28,1)\n",
    "x_reshaped_valid = x_valid_images.reshape(-1,28,28,1)\n",
    "test_reshaped = test_images.reshape(-1,28,28,1)\n",
    "\n",
    "init = keras.initializers.HeNormal(seed=42) # best for ReLu\n",
    "con_init = keras.initializers.GlorotUniform(seed=42) # default ma con seed fissato per ripetibilit \n",
    "\n",
    "CNN_lenet = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), activation='relu',padding='same',input_shape=(28,28,1), kernel_initializer=con_init),\n",
    "    AveragePooling2D(pool_size=(2, 2),strides=2),\n",
    "    Conv2D(20, kernel_size=(5, 5), activation='relu', kernel_initializer=con_init),\n",
    "    AveragePooling2D(pool_size=(2, 2),strides=2),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_initializer=init),\n",
    "    Dense(128, activation='relu', kernel_initializer=init),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "CNN_lenet.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_cnn_lenet = CNN_lenet.fit(x_reshaped_train, x_train_labels, epochs=300, validation_data=(x_reshaped_valid, x_valid_labels), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85c1e8d5-0f35-4f68-a61c-befa8f1ef000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9714\n",
      "CNN_lenet_accuracy over test set =  0.9713740348815918\n"
     ]
    }
   ],
   "source": [
    "#test set\n",
    "cnn_lenet_loss, cnn_lenet_accuracy = CNN_lenet.evaluate(test_reshaped, test_labels)\n",
    "print('CNN_lenet_accuracy over test set = ', cnn_lenet_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5bd5cd3-d28b-45f3-ae7d-e2f490d97d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 10)        260       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 14, 14, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 20)        5020      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 5, 5, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 166,561\n",
      "Trainable params: 166,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "add_result_to_csv('CNN LeNet', cnn_lenet_accuracy)\n",
    "CNN_lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48290ce-3695-41b8-a3f6-db2d169f158d",
   "metadata": {},
   "source": [
    "## CNN + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "981bf310-a21d-4718-b069-e34f68a22c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "118/118 [==============================] - 2s 12ms/step - loss: 0.5798 - accuracy: 0.7315 - val_loss: 0.5498 - val_accuracy: 0.7487\n",
      "Epoch 2/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.4325 - accuracy: 0.7811 - val_loss: 0.2514 - val_accuracy: 0.9109\n",
      "Epoch 3/300\n",
      "118/118 [==============================] - 2s 13ms/step - loss: 0.2829 - accuracy: 0.8786 - val_loss: 0.2021 - val_accuracy: 0.9183\n",
      "Epoch 4/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.2454 - accuracy: 0.9017 - val_loss: 0.1597 - val_accuracy: 0.9427\n",
      "Epoch 5/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.2151 - accuracy: 0.9155 - val_loss: 0.1938 - val_accuracy: 0.9268\n",
      "Epoch 6/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.2079 - accuracy: 0.9201 - val_loss: 0.1453 - val_accuracy: 0.9427\n",
      "Epoch 7/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9262 - val_loss: 0.1291 - val_accuracy: 0.9512\n",
      "Epoch 8/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1810 - accuracy: 0.9301 - val_loss: 0.1461 - val_accuracy: 0.9459\n",
      "Epoch 9/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1834 - accuracy: 0.9283 - val_loss: 0.1320 - val_accuracy: 0.9544\n",
      "Epoch 10/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1772 - accuracy: 0.9341 - val_loss: 0.1197 - val_accuracy: 0.9533\n",
      "Epoch 11/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1672 - accuracy: 0.9373 - val_loss: 0.1243 - val_accuracy: 0.9502\n",
      "Epoch 12/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1668 - accuracy: 0.9349 - val_loss: 0.1237 - val_accuracy: 0.9565\n",
      "Epoch 13/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1693 - accuracy: 0.9344 - val_loss: 0.1017 - val_accuracy: 0.9671\n",
      "Epoch 14/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1535 - accuracy: 0.9405 - val_loss: 0.1145 - val_accuracy: 0.9555\n",
      "Epoch 15/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1536 - accuracy: 0.9424 - val_loss: 0.1294 - val_accuracy: 0.9523\n",
      "Epoch 16/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1506 - accuracy: 0.9410 - val_loss: 0.0963 - val_accuracy: 0.9650\n",
      "Epoch 17/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1517 - accuracy: 0.9426 - val_loss: 0.0892 - val_accuracy: 0.9661\n",
      "Epoch 18/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1477 - accuracy: 0.9437 - val_loss: 0.0886 - val_accuracy: 0.9703\n",
      "Epoch 19/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1391 - accuracy: 0.9482 - val_loss: 0.1075 - val_accuracy: 0.9608\n",
      "Epoch 20/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1296 - accuracy: 0.9511 - val_loss: 0.1216 - val_accuracy: 0.9533\n",
      "Epoch 21/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1328 - accuracy: 0.9495 - val_loss: 0.0912 - val_accuracy: 0.9661\n",
      "Epoch 22/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1301 - accuracy: 0.9538 - val_loss: 0.0888 - val_accuracy: 0.9639\n",
      "Epoch 23/300\n",
      "118/118 [==============================] - 1s 8ms/step - loss: 0.1302 - accuracy: 0.9509 - val_loss: 0.0823 - val_accuracy: 0.9745\n",
      "Epoch 24/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1329 - accuracy: 0.9495 - val_loss: 0.0779 - val_accuracy: 0.9745\n",
      "Epoch 25/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1270 - accuracy: 0.9538 - val_loss: 0.0950 - val_accuracy: 0.9703\n",
      "Epoch 26/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1288 - accuracy: 0.9509 - val_loss: 0.0808 - val_accuracy: 0.9745\n",
      "Epoch 27/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1185 - accuracy: 0.9551 - val_loss: 0.0802 - val_accuracy: 0.9745\n",
      "Epoch 28/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1359 - accuracy: 0.9509 - val_loss: 0.0921 - val_accuracy: 0.9661\n",
      "Epoch 29/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1146 - accuracy: 0.9580 - val_loss: 0.0834 - val_accuracy: 0.9682\n",
      "Epoch 30/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1198 - accuracy: 0.9543 - val_loss: 0.0778 - val_accuracy: 0.9703\n",
      "Epoch 31/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1200 - accuracy: 0.9514 - val_loss: 0.0772 - val_accuracy: 0.9799\n",
      "Epoch 32/300\n",
      "118/118 [==============================] - 1s 12ms/step - loss: 0.1294 - accuracy: 0.9509 - val_loss: 0.0774 - val_accuracy: 0.9788\n",
      "Epoch 33/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1229 - accuracy: 0.9541 - val_loss: 0.0853 - val_accuracy: 0.9661\n",
      "Epoch 34/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1320 - accuracy: 0.9556 - val_loss: 0.0834 - val_accuracy: 0.9735\n",
      "Epoch 35/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1152 - accuracy: 0.9586 - val_loss: 0.0724 - val_accuracy: 0.9756\n",
      "Epoch 36/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1099 - accuracy: 0.9591 - val_loss: 0.0726 - val_accuracy: 0.9735\n",
      "Epoch 37/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1194 - accuracy: 0.9572 - val_loss: 0.0825 - val_accuracy: 0.9703\n",
      "Epoch 38/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1110 - accuracy: 0.9583 - val_loss: 0.0739 - val_accuracy: 0.9767\n",
      "Epoch 39/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1133 - accuracy: 0.9599 - val_loss: 0.0731 - val_accuracy: 0.9767\n",
      "Epoch 40/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1098 - accuracy: 0.9631 - val_loss: 0.0697 - val_accuracy: 0.9777\n",
      "Epoch 41/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1100 - accuracy: 0.9602 - val_loss: 0.0735 - val_accuracy: 0.9735\n",
      "Epoch 42/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1097 - accuracy: 0.9594 - val_loss: 0.0774 - val_accuracy: 0.9777\n",
      "Epoch 43/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1130 - accuracy: 0.9564 - val_loss: 0.0754 - val_accuracy: 0.9745\n",
      "Epoch 44/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1083 - accuracy: 0.9586 - val_loss: 0.0918 - val_accuracy: 0.9692\n",
      "Epoch 45/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1099 - accuracy: 0.9588 - val_loss: 0.0685 - val_accuracy: 0.9724\n",
      "Epoch 46/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1008 - accuracy: 0.9610 - val_loss: 0.0918 - val_accuracy: 0.9618\n",
      "Epoch 47/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1090 - accuracy: 0.9602 - val_loss: 0.0756 - val_accuracy: 0.9724\n",
      "Epoch 48/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.0975 - accuracy: 0.9612 - val_loss: 0.0706 - val_accuracy: 0.9767\n",
      "Epoch 49/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0993 - accuracy: 0.9623 - val_loss: 0.0817 - val_accuracy: 0.9735\n",
      "Epoch 50/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0899 - accuracy: 0.9663 - val_loss: 0.0667 - val_accuracy: 0.9777\n",
      "Epoch 51/300\n",
      "118/118 [==============================] - 1s 11ms/step - loss: 0.1121 - accuracy: 0.9588 - val_loss: 0.0779 - val_accuracy: 0.9703\n",
      "Epoch 52/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1030 - accuracy: 0.9602 - val_loss: 0.0720 - val_accuracy: 0.9767\n",
      "Epoch 53/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0998 - accuracy: 0.9639 - val_loss: 0.0721 - val_accuracy: 0.9777\n",
      "Epoch 54/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0992 - accuracy: 0.9620 - val_loss: 0.0757 - val_accuracy: 0.9714\n",
      "Epoch 55/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1043 - accuracy: 0.9639 - val_loss: 0.0815 - val_accuracy: 0.9703\n",
      "Epoch 56/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.1084 - accuracy: 0.9607 - val_loss: 0.0698 - val_accuracy: 0.9777\n",
      "Epoch 57/300\n",
      "118/118 [==============================] - 1s 9ms/step - loss: 0.0998 - accuracy: 0.9647 - val_loss: 0.0698 - val_accuracy: 0.9756\n",
      "Epoch 58/300\n",
      "118/118 [==============================] - 1s 12ms/step - loss: 0.0954 - accuracy: 0.9641 - val_loss: 0.0673 - val_accuracy: 0.9724\n",
      "Epoch 59/300\n",
      "118/118 [==============================] - 1s 12ms/step - loss: 0.1188 - accuracy: 0.9562 - val_loss: 0.1064 - val_accuracy: 0.9608\n",
      "Epoch 60/300\n",
      "118/118 [==============================] - 1s 10ms/step - loss: 0.1020 - accuracy: 0.9628 - val_loss: 0.0688 - val_accuracy: 0.9799\n"
     ]
    }
   ],
   "source": [
    "x_reshaped_train = x_train_images.reshape(-1,28,28,1)\n",
    "x_reshaped_valid = x_valid_images.reshape(-1,28,28,1)\n",
    "test_reshaped = test_images.reshape(-1,28,28,1)\n",
    "\n",
    "init = keras.initializers.HeNormal(seed=42)\n",
    "\n",
    "CNN_lenet_opt = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), activation='relu',padding='same',input_shape=(28,28,1)),\n",
    "    AveragePooling2D(pool_size=(2, 2),strides=2),        \n",
    "    Dropout(0.35),\n",
    "    Conv2D(20, kernel_size=(5, 5), activation='relu'),\n",
    "    AveragePooling2D(pool_size=(2, 2),strides=2),\n",
    "    Dropout(0.35),\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu', kernel_initializer=init),\n",
    "    Dropout(0.7),\n",
    "    Dense(128, activation='relu', kernel_initializer=init),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "CNN_lenet_opt.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_cnn_lenet = CNN_lenet_opt.fit(x_reshaped_train, x_train_labels, epochs=300, validation_data=(x_reshaped_valid, x_valid_labels), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fd6d265-5e33-4ebf-9f11-28145a8d20ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9809\n",
      "CNN_lenet_opt_accuracy over test set =  0.9809160232543945\n"
     ]
    }
   ],
   "source": [
    "#test set\n",
    "CNN_lenet_opt_loss, CNN_lenet_opt_accuracy = CNN_lenet_opt.evaluate(test_reshaped, test_labels)\n",
    "print('CNN_lenet_opt_accuracy over test set = ', CNN_lenet_opt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f48c2ab-7657-4121-986f-6f7aaaf773e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 10)        260       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 14, 14, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 20)        5020      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 5, 5, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               128256    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 166,561\n",
      "Trainable params: 166,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "add_result_to_csv('CNN LeNet Dropout', CNN_lenet_opt_accuracy)\n",
    "CNN_lenet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0da466-b39f-4ee8-a9b7-f326752aa2b4",
   "metadata": {},
   "source": [
    "### CNN + Dropout + Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06823d26-f8a8-4b2d-bf50-14e9550d24f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 2s 20ms/step - loss: 0.8006 - accuracy: 0.6100 - val_loss: 0.5761 - val_accuracy: 0.7449\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5111 - accuracy: 0.7710 - val_loss: 0.5228 - val_accuracy: 0.7449\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.3680 - accuracy: 0.8500 - val_loss: 0.5207 - val_accuracy: 0.7449\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3091 - accuracy: 0.8670 - val_loss: 0.5205 - val_accuracy: 0.7449\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.2810 - accuracy: 0.8890 - val_loss: 0.4868 - val_accuracy: 0.7449\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.2809 - accuracy: 0.8920 - val_loss: 0.4893 - val_accuracy: 0.7449\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.2562 - accuracy: 0.9020 - val_loss: 0.6266 - val_accuracy: 0.7449\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.2723 - accuracy: 0.9040 - val_loss: 0.2933 - val_accuracy: 0.9253\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.2495 - accuracy: 0.9020 - val_loss: 0.2446 - val_accuracy: 0.9035\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.2175 - accuracy: 0.9100 - val_loss: 0.4118 - val_accuracy: 0.7913\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.2319 - accuracy: 0.9110 - val_loss: 0.2481 - val_accuracy: 0.8857\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.2157 - accuracy: 0.9080 - val_loss: 0.2041 - val_accuracy: 0.9320\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.2129 - accuracy: 0.9180 - val_loss: 0.2055 - val_accuracy: 0.9331\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.2116 - accuracy: 0.9190 - val_loss: 0.1891 - val_accuracy: 0.9264\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.1803 - accuracy: 0.9310 - val_loss: 0.3399 - val_accuracy: 0.8301\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1873 - accuracy: 0.9240 - val_loss: 0.4640 - val_accuracy: 0.7476\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1693 - accuracy: 0.9310 - val_loss: 0.1712 - val_accuracy: 0.9328\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.2060 - accuracy: 0.9190 - val_loss: 0.4676 - val_accuracy: 0.7624\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1976 - accuracy: 0.9160 - val_loss: 0.1332 - val_accuracy: 0.9485\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1795 - accuracy: 0.9280 - val_loss: 0.3684 - val_accuracy: 0.8109\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1573 - accuracy: 0.9380 - val_loss: 0.6823 - val_accuracy: 0.6624\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1550 - accuracy: 0.9440 - val_loss: 0.2768 - val_accuracy: 0.8581\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1948 - accuracy: 0.9260 - val_loss: 0.1707 - val_accuracy: 0.9372\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.1645 - accuracy: 0.9420 - val_loss: 0.1628 - val_accuracy: 0.9355\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1554 - accuracy: 0.9370 - val_loss: 0.1235 - val_accuracy: 0.9547\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1496 - accuracy: 0.9530 - val_loss: 0.1784 - val_accuracy: 0.9221\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1583 - accuracy: 0.9410 - val_loss: 0.6625 - val_accuracy: 0.8441\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1452 - accuracy: 0.9480 - val_loss: 0.1896 - val_accuracy: 0.9250\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1635 - accuracy: 0.9370 - val_loss: 0.3135 - val_accuracy: 0.8436\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1590 - accuracy: 0.9430 - val_loss: 1.0714 - val_accuracy: 0.5200\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1582 - accuracy: 0.9400 - val_loss: 0.3264 - val_accuracy: 0.8840\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.1405 - accuracy: 0.9460 - val_loss: 0.1183 - val_accuracy: 0.9552\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1694 - accuracy: 0.9320 - val_loss: 0.5472 - val_accuracy: 0.6958\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1407 - accuracy: 0.9530 - val_loss: 0.2284 - val_accuracy: 0.9199\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1517 - accuracy: 0.9440 - val_loss: 0.5319 - val_accuracy: 0.8409\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1519 - accuracy: 0.9370 - val_loss: 0.2000 - val_accuracy: 0.9272\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1507 - accuracy: 0.9440 - val_loss: 2.0237 - val_accuracy: 0.2859\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.1375 - accuracy: 0.9500 - val_loss: 1.4250 - val_accuracy: 0.3716\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1695 - accuracy: 0.9400 - val_loss: 0.7161 - val_accuracy: 0.6386\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 0.1433 - accuracy: 0.9420 - val_loss: 0.1232 - val_accuracy: 0.9550\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.1444 - accuracy: 0.9420 - val_loss: 0.2811 - val_accuracy: 0.8762\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.1270 - accuracy: 0.9530 - val_loss: 0.1520 - val_accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "# Reshape dei dati di input\n",
    "x_reshaped_train = x_train_images.reshape(-1, 28, 28, 1)\n",
    "x_reshaped_valid = x_valid_images.reshape(-1, 28, 28, 1)\n",
    "test_reshaped = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Inizializzazione dei pesi\n",
    "init = HeNormal(seed=42)\n",
    "\n",
    "# Costruzione della rete con batch normalization\n",
    "CNN_lenet_opt_bn = Sequential([\n",
    "    Conv2D(10, kernel_size=(5, 5), padding='same', input_shape=(28, 28, 1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Conv2D(20, kernel_size=(5, 5)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    AveragePooling2D(pool_size=(2, 2), strides=2),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(256, kernel_initializer=init),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(128, kernel_initializer=init),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Compilazione del modello\n",
    "CNN_lenet_opt_bn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestramento del modello\n",
    "history_cnn_lenet_bn = CNN_lenet_opt_bn.fit(x_reshaped_train, x_train_labels, epochs=300, validation_data=(x_reshaped_valid, x_valid_labels), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f74346c-32b0-4b21-8772-31912bfa498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.9656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10544075071811676, 0.9656488299369812]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set\n",
    "CNN_lenet_opt_bn.evaluate(test_reshaped, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92402a3-10a9-420d-ae03-7bd2724f340e",
   "metadata": {},
   "source": [
    "Mentre il dropout sembra aver migliorato leggermente l'accuracy, la Batch Normalization non sembra avere lo stesso effetto, avendo una accuracy sul testing set leggermente pi bassa rispetto al modello precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e6c2e-2a6e-4361-a8d4-4a4843848ad9",
   "metadata": {},
   "source": [
    "## CNN con blocco Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "824b516b-6c95-4666-bff8-9de5b8bcde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per creare un blocco Inception\n",
    "def single_inception_block(x, filters):\n",
    "    layers = tf.keras.layers\n",
    "    # Branch 1: 1x1 Convolution\n",
    "    branch1 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Branch 2: 1x1 Convolution followed by 3x3 Convolution\n",
    "    branch2 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch2 = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(branch2)\n",
    "    \n",
    "    # Branch 3: 1x1 Convolution followed by 5x5 Convolution\n",
    "    branch3 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3 = layers.Conv2D(filters, (5, 5), padding='same', activation='relu')(branch3)\n",
    "    \n",
    "    # Branch 4: 3x3 MaxPooling followed by 1x1 Convolution\n",
    "    branch4 = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch4 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(branch4)\n",
    "    \n",
    "    # Concatenate all branches\n",
    "    output = layers.concatenate([branch1, branch2, branch3, branch4], axis=-1)\n",
    "    return output\n",
    "\n",
    "# Costruzione del modello\n",
    "def create_single_inception_cnn(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    layers = tf.keras.layers\n",
    "\n",
    "    \n",
    "    # blocco Inception\n",
    "    x = single_inception_block(inputs, 128)\n",
    "    \n",
    "    # Aggiunta di un Global Average Pooling per ridurre la dimensionalit\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Aggiunta di livelli densi per la classificazione\n",
    "    init = keras.initializers.HeNormal(seed=42)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu', kernel_initializer=init)(x)\n",
    "    x = layers.Dense(128, activation='relu', kernel_initializer=init)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Creazione del modello con input shape (28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "CNN_single_inception = create_single_inception_cnn(input_shape)\n",
    "\n",
    "# Compilazione del modello\n",
    "CNN_single_inception.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Riassunto del modello\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69512ae4-340b-4cc0-9ced-587724cc89d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 3s 45ms/step - loss: 0.5861 - accuracy: 0.7290 - val_loss: 0.5622 - val_accuracy: 0.7449\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.5841 - accuracy: 0.7290 - val_loss: 0.5697 - val_accuracy: 0.7449\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.5880 - accuracy: 0.7290 - val_loss: 0.5725 - val_accuracy: 0.7449\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.5868 - accuracy: 0.7290 - val_loss: 0.5598 - val_accuracy: 0.7449\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.5775 - accuracy: 0.7290 - val_loss: 0.5575 - val_accuracy: 0.7449\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.5770 - accuracy: 0.7290 - val_loss: 0.5599 - val_accuracy: 0.7449\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.5714 - accuracy: 0.7290 - val_loss: 0.5510 - val_accuracy: 0.7449\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.5725 - accuracy: 0.7290 - val_loss: 0.5634 - val_accuracy: 0.7449\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.5658 - accuracy: 0.7290 - val_loss: 0.5952 - val_accuracy: 0.7449\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.5684 - accuracy: 0.7290 - val_loss: 0.5401 - val_accuracy: 0.7449\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.5423 - accuracy: 0.7290 - val_loss: 0.5003 - val_accuracy: 0.7449\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.5027 - accuracy: 0.7510 - val_loss: 0.4821 - val_accuracy: 0.7524\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.4414 - accuracy: 0.8010 - val_loss: 0.3700 - val_accuracy: 0.8352\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3683 - accuracy: 0.8500 - val_loss: 0.3820 - val_accuracy: 0.8401\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.3470 - accuracy: 0.8480 - val_loss: 0.3170 - val_accuracy: 0.8614\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3232 - accuracy: 0.8560 - val_loss: 0.5379 - val_accuracy: 0.7352\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3831 - accuracy: 0.8250 - val_loss: 0.3270 - val_accuracy: 0.8581\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3092 - accuracy: 0.8780 - val_loss: 0.3051 - val_accuracy: 0.8587\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3068 - accuracy: 0.8660 - val_loss: 0.2965 - val_accuracy: 0.8676\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3219 - accuracy: 0.8690 - val_loss: 0.2963 - val_accuracy: 0.8638\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2951 - accuracy: 0.8710 - val_loss: 0.3187 - val_accuracy: 0.8536\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3491 - accuracy: 0.8450 - val_loss: 0.3080 - val_accuracy: 0.8617\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3243 - accuracy: 0.8500 - val_loss: 0.2913 - val_accuracy: 0.8692\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2919 - accuracy: 0.8760 - val_loss: 0.3181 - val_accuracy: 0.8560\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2878 - accuracy: 0.8800 - val_loss: 0.2835 - val_accuracy: 0.8684\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2954 - accuracy: 0.8760 - val_loss: 0.3636 - val_accuracy: 0.8363\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.3015 - accuracy: 0.8760 - val_loss: 0.2867 - val_accuracy: 0.8687\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3060 - accuracy: 0.8710 - val_loss: 0.2993 - val_accuracy: 0.8662\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3027 - accuracy: 0.8710 - val_loss: 0.3385 - val_accuracy: 0.8417\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.3016 - accuracy: 0.8820 - val_loss: 0.2801 - val_accuracy: 0.8689\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.3051 - accuracy: 0.8670 - val_loss: 0.3934 - val_accuracy: 0.8120\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2903 - accuracy: 0.8670 - val_loss: 0.2797 - val_accuracy: 0.8719\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2801 - accuracy: 0.8950 - val_loss: 0.2862 - val_accuracy: 0.8700\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.2717 - accuracy: 0.8790 - val_loss: 0.2729 - val_accuracy: 0.8722\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2844 - accuracy: 0.8810 - val_loss: 0.2832 - val_accuracy: 0.8762\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2680 - accuracy: 0.8900 - val_loss: 0.2719 - val_accuracy: 0.8800\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2800 - accuracy: 0.8840 - val_loss: 0.3013 - val_accuracy: 0.8689\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2704 - accuracy: 0.8890 - val_loss: 0.2905 - val_accuracy: 0.8687\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2776 - accuracy: 0.8720 - val_loss: 0.2975 - val_accuracy: 0.8741\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2716 - accuracy: 0.8920 - val_loss: 0.2702 - val_accuracy: 0.8824\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2982 - accuracy: 0.8710 - val_loss: 0.2666 - val_accuracy: 0.8800\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2703 - accuracy: 0.8900 - val_loss: 0.3053 - val_accuracy: 0.8635\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2738 - accuracy: 0.8880 - val_loss: 0.2923 - val_accuracy: 0.8703\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2683 - accuracy: 0.8990 - val_loss: 0.2931 - val_accuracy: 0.8681\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2525 - accuracy: 0.8990 - val_loss: 0.2555 - val_accuracy: 0.8902\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2541 - accuracy: 0.9020 - val_loss: 0.2677 - val_accuracy: 0.8859\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2680 - accuracy: 0.8920 - val_loss: 0.3097 - val_accuracy: 0.8557\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2782 - accuracy: 0.8870 - val_loss: 0.2958 - val_accuracy: 0.8660\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2528 - accuracy: 0.8960 - val_loss: 0.2564 - val_accuracy: 0.8900\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.2435 - accuracy: 0.9100 - val_loss: 0.2463 - val_accuracy: 0.8943\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2512 - accuracy: 0.8990 - val_loss: 0.2470 - val_accuracy: 0.8929\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2688 - accuracy: 0.8870 - val_loss: 0.2450 - val_accuracy: 0.8929\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.2439 - accuracy: 0.9080 - val_loss: 0.2740 - val_accuracy: 0.8789\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2453 - accuracy: 0.9000 - val_loss: 0.2439 - val_accuracy: 0.8946\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2667 - accuracy: 0.9030 - val_loss: 0.2611 - val_accuracy: 0.8843\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2401 - accuracy: 0.8930 - val_loss: 0.2420 - val_accuracy: 0.8999\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2624 - accuracy: 0.8950 - val_loss: 0.2680 - val_accuracy: 0.8838\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2383 - accuracy: 0.9070 - val_loss: 0.2605 - val_accuracy: 0.8816\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2420 - accuracy: 0.9050 - val_loss: 0.2302 - val_accuracy: 0.9061\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2245 - accuracy: 0.9090 - val_loss: 0.2278 - val_accuracy: 0.9061\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2412 - accuracy: 0.8960 - val_loss: 0.2848 - val_accuracy: 0.8708\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2263 - accuracy: 0.9070 - val_loss: 0.2455 - val_accuracy: 0.8972\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2411 - accuracy: 0.9060 - val_loss: 0.2501 - val_accuracy: 0.8873\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2262 - accuracy: 0.9010 - val_loss: 0.2182 - val_accuracy: 0.9121\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2511 - accuracy: 0.9010 - val_loss: 0.2275 - val_accuracy: 0.9078\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2304 - accuracy: 0.9120 - val_loss: 0.2506 - val_accuracy: 0.8886\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.2147 - accuracy: 0.9090 - val_loss: 0.2282 - val_accuracy: 0.9045\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2231 - accuracy: 0.9220 - val_loss: 0.2923 - val_accuracy: 0.8652\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.2304 - accuracy: 0.9080 - val_loss: 0.2121 - val_accuracy: 0.9126\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2232 - accuracy: 0.9070 - val_loss: 0.2139 - val_accuracy: 0.9115\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.2071 - accuracy: 0.9180 - val_loss: 0.2058 - val_accuracy: 0.9180\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2090 - accuracy: 0.9130 - val_loss: 0.2225 - val_accuracy: 0.9045\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.2196 - accuracy: 0.9100 - val_loss: 0.2028 - val_accuracy: 0.9194\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2150 - accuracy: 0.9030 - val_loss: 0.2501 - val_accuracy: 0.8983\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2219 - accuracy: 0.9120 - val_loss: 0.2010 - val_accuracy: 0.9226\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1969 - accuracy: 0.9200 - val_loss: 0.2021 - val_accuracy: 0.9186\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1922 - accuracy: 0.9260 - val_loss: 0.2033 - val_accuracy: 0.9177\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2073 - accuracy: 0.9150 - val_loss: 0.2484 - val_accuracy: 0.8854\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2251 - accuracy: 0.9040 - val_loss: 0.2192 - val_accuracy: 0.9110\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.2004 - accuracy: 0.9150 - val_loss: 0.1974 - val_accuracy: 0.9194\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1894 - accuracy: 0.9280 - val_loss: 0.2352 - val_accuracy: 0.8932\n",
      "Epoch 82/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1877 - accuracy: 0.9330 - val_loss: 0.1875 - val_accuracy: 0.9258\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1905 - accuracy: 0.9250 - val_loss: 0.1898 - val_accuracy: 0.9215\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1794 - accuracy: 0.9310 - val_loss: 0.1774 - val_accuracy: 0.9318\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1784 - accuracy: 0.9330 - val_loss: 0.2450 - val_accuracy: 0.8875\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1906 - accuracy: 0.9220 - val_loss: 0.2273 - val_accuracy: 0.8983\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.1853 - accuracy: 0.9240 - val_loss: 0.1790 - val_accuracy: 0.9285\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.2273 - accuracy: 0.9070 - val_loss: 0.2455 - val_accuracy: 0.9029\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1999 - accuracy: 0.9200 - val_loss: 0.1906 - val_accuracy: 0.9180\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1798 - accuracy: 0.9270 - val_loss: 0.1745 - val_accuracy: 0.9331\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1758 - accuracy: 0.9330 - val_loss: 0.1823 - val_accuracy: 0.9215\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1967 - accuracy: 0.9250 - val_loss: 0.1970 - val_accuracy: 0.9110\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1680 - accuracy: 0.9370 - val_loss: 0.1710 - val_accuracy: 0.9328\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1812 - accuracy: 0.9230 - val_loss: 0.2469 - val_accuracy: 0.9032\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.2002 - accuracy: 0.9210 - val_loss: 0.1700 - val_accuracy: 0.9342\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1685 - accuracy: 0.9360 - val_loss: 0.1724 - val_accuracy: 0.9323\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1613 - accuracy: 0.9390 - val_loss: 0.1655 - val_accuracy: 0.9393\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1734 - accuracy: 0.9280 - val_loss: 0.1878 - val_accuracy: 0.9296\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1606 - accuracy: 0.9350 - val_loss: 0.1639 - val_accuracy: 0.9396\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1650 - accuracy: 0.9330 - val_loss: 0.1706 - val_accuracy: 0.9331\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1712 - accuracy: 0.9330 - val_loss: 0.1751 - val_accuracy: 0.9382\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1721 - accuracy: 0.9310 - val_loss: 0.1669 - val_accuracy: 0.9369\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1657 - accuracy: 0.9350 - val_loss: 0.1989 - val_accuracy: 0.9097\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1687 - accuracy: 0.9340 - val_loss: 0.1638 - val_accuracy: 0.9361\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1568 - accuracy: 0.9340 - val_loss: 0.1847 - val_accuracy: 0.9339\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.1547 - accuracy: 0.9360 - val_loss: 0.1586 - val_accuracy: 0.9382\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1859 - accuracy: 0.9250 - val_loss: 0.1642 - val_accuracy: 0.9358\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1589 - accuracy: 0.9340 - val_loss: 0.1822 - val_accuracy: 0.9361\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.1803 - accuracy: 0.9330 - val_loss: 0.2357 - val_accuracy: 0.9137\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.1656 - accuracy: 0.9300 - val_loss: 0.1724 - val_accuracy: 0.9328\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 0.1652 - accuracy: 0.9370 - val_loss: 0.1550 - val_accuracy: 0.9415\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1656 - accuracy: 0.9350 - val_loss: 0.1631 - val_accuracy: 0.9434\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1571 - accuracy: 0.9370 - val_loss: 0.2151 - val_accuracy: 0.9072\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.1652 - accuracy: 0.9310 - val_loss: 0.1617 - val_accuracy: 0.9380\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1463 - accuracy: 0.9400 - val_loss: 0.1628 - val_accuracy: 0.9436\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1681 - accuracy: 0.9260 - val_loss: 0.1769 - val_accuracy: 0.9345\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1528 - accuracy: 0.9430 - val_loss: 0.1653 - val_accuracy: 0.9409\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1570 - accuracy: 0.9370 - val_loss: 0.1561 - val_accuracy: 0.9391\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 0.1537 - accuracy: 0.9380 - val_loss: 0.1906 - val_accuracy: 0.9355\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 0.1803 - accuracy: 0.9290 - val_loss: 0.2285 - val_accuracy: 0.8954\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 0.2028 - accuracy: 0.9260 - val_loss: 0.1734 - val_accuracy: 0.9248\n"
     ]
    }
   ],
   "source": [
    "# Creazione del callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello\n",
    "history_single_inception = CNN_single_inception.fit(x_train_images, x_train_labels, epochs=300,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid_images, x_valid_labels),\n",
    "                    callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cf51c0e-62b2-4049-82fd-5124c2b82cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1405 - accuracy: 0.9504\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 128)  256         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 128)  256         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 1)    0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 28, 28, 128)  256         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 128)  147584      conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 28, 28, 128)  409728      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 28, 28, 128)  256         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 28, 28, 512)  0           conv2d_20[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 512)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          131328      global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            129         dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 722,689\n",
      "Trainable params: 722,689\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_single_inception.evaluate(test_images, test_labels)\n",
    "CNN_single_inception.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4498b6-985d-4bae-9a1a-79df7da0ce57",
   "metadata": {},
   "source": [
    "L'idea  quella di costruire una CNN che richiami quella sviluppata da Google (GoogLeNet) andando a sfruttare il blocco transformer. Si opera questa scelta perch una rete del genere ha mostrato ottime prestazioni in passato, e per non dover cercare di ottimizzare il tipo di Pooling e la dimensione dei Kernel, andandone a sfruttare di diverse dimensioni all'interno del blocco Inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "223fcb3f-1060-48ee-998e-39e03ba695de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per creare un blocco Inception\n",
    "def inception_module(x, filters):\n",
    "    layers = tf.keras.layers\n",
    "    # Branch 1: 1x1 Convolution\n",
    "    branch1 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Branch 2: 1x1 Convolution followed by 3x3 Convolution\n",
    "    branch2 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch2 = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(branch2)\n",
    "    \n",
    "    # Branch 3: 1x1 Convolution followed by 5x5 Convolution\n",
    "    branch3 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(x)\n",
    "    branch3 = layers.Conv2D(filters, (5, 5), padding='same', activation='relu')(branch3)\n",
    "    \n",
    "    # Branch 4: 3x3 MaxPooling followed by 1x1 Convolution\n",
    "    branch4 = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch4 = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(branch4)\n",
    "    \n",
    "    # Concatenate all branches\n",
    "    output = layers.concatenate([branch1, branch2, branch3, branch4], axis=-1)\n",
    "    return output\n",
    "\n",
    "# Costruzione del modello\n",
    "def create_inception_cnn(input_shape):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    layers = tf.keras.layers\n",
    "\n",
    "    # Primo blocco Inception\n",
    "    x = inception_module(inputs, 32)\n",
    "\n",
    "    # Secondo blocco Inception\n",
    "    x = inception_module(x, 64)\n",
    "\n",
    "    # Terzo blocco Inception\n",
    "    x = inception_module(x, 128)\n",
    "\n",
    "    # Aggiunta di un Global Average Pooling per ridurre la dimensionalit\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Aggiunta di un livello denso per la classificazione\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Creazione del modello con input shape (28, 28, 1) per immagini in scala di grigi\n",
    "input_shape = (28, 28, 1)\n",
    "cnn_inception_extended = create_inception_cnn(input_shape)\n",
    "\n",
    "# Compilazione del modello\n",
    "cnn_inception_extended.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Riassunto del modello\n",
    "#cnn_inception_extended.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "538d97c0-6361-4e59-b813-36c6d3934075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "32/32 [==============================] - 4s 68ms/step - loss: 0.6088 - accuracy: 0.7190 - val_loss: 0.5645 - val_accuracy: 0.7449\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.5958 - accuracy: 0.7290 - val_loss: 0.5606 - val_accuracy: 0.7449\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.5884 - accuracy: 0.7290 - val_loss: 0.5878 - val_accuracy: 0.7449\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.5421 - accuracy: 0.7290 - val_loss: 0.6724 - val_accuracy: 0.7505\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.5649 - accuracy: 0.7030 - val_loss: 0.4065 - val_accuracy: 0.7449\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.6022 - accuracy: 0.6980 - val_loss: 0.5879 - val_accuracy: 0.7449\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.6128 - accuracy: 0.7290 - val_loss: 0.6056 - val_accuracy: 0.7449\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.5875 - accuracy: 0.7290 - val_loss: 0.5375 - val_accuracy: 0.7449\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.5846 - accuracy: 0.7290 - val_loss: 0.5826 - val_accuracy: 0.7449\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.6047 - accuracy: 0.7290 - val_loss: 0.5727 - val_accuracy: 0.7449\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.5536 - accuracy: 0.7290 - val_loss: 0.5139 - val_accuracy: 0.7449\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.5783 - accuracy: 0.7060 - val_loss: 0.6081 - val_accuracy: 0.7449\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.5336 - accuracy: 0.7290 - val_loss: 0.6124 - val_accuracy: 0.7449\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.4894 - accuracy: 0.7290 - val_loss: 0.3810 - val_accuracy: 0.7451\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.4105 - accuracy: 0.7830 - val_loss: 0.4686 - val_accuracy: 0.6891\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.3913 - accuracy: 0.7900 - val_loss: 0.3259 - val_accuracy: 0.8522\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.4579 - accuracy: 0.7820 - val_loss: 0.3352 - val_accuracy: 0.8490\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.3616 - accuracy: 0.8490 - val_loss: 0.3122 - val_accuracy: 0.8627\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.3948 - accuracy: 0.8320 - val_loss: 0.3686 - val_accuracy: 0.8622\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.3180 - accuracy: 0.8760 - val_loss: 0.3194 - val_accuracy: 0.8401\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.3931 - accuracy: 0.8200 - val_loss: 0.3794 - val_accuracy: 0.8592\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.3481 - accuracy: 0.8380 - val_loss: 0.3189 - val_accuracy: 0.8662\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.3414 - accuracy: 0.8580 - val_loss: 0.3020 - val_accuracy: 0.8541\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.3256 - accuracy: 0.8540 - val_loss: 0.3057 - val_accuracy: 0.8603\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.3001 - accuracy: 0.8770 - val_loss: 0.4462 - val_accuracy: 0.7705\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.3287 - accuracy: 0.8530 - val_loss: 0.2837 - val_accuracy: 0.8765\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2992 - accuracy: 0.8750 - val_loss: 0.3044 - val_accuracy: 0.8538\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2770 - accuracy: 0.8850 - val_loss: 0.3145 - val_accuracy: 0.8724\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.3168 - accuracy: 0.8600 - val_loss: 0.2398 - val_accuracy: 0.8972\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2554 - accuracy: 0.8980 - val_loss: 0.2482 - val_accuracy: 0.8948\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2538 - accuracy: 0.8980 - val_loss: 0.1957 - val_accuracy: 0.9153\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1992 - accuracy: 0.9170 - val_loss: 0.2119 - val_accuracy: 0.9067\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2402 - accuracy: 0.9050 - val_loss: 0.1956 - val_accuracy: 0.9226\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.2491 - accuracy: 0.8930 - val_loss: 0.2335 - val_accuracy: 0.9032\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2081 - accuracy: 0.9160 - val_loss: 0.2246 - val_accuracy: 0.9008\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2217 - accuracy: 0.9110 - val_loss: 0.1756 - val_accuracy: 0.9261\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.2127 - accuracy: 0.9170 - val_loss: 0.2011 - val_accuracy: 0.9215\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.2191 - accuracy: 0.9170 - val_loss: 0.2775 - val_accuracy: 0.8808\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.2100 - accuracy: 0.9170 - val_loss: 0.1746 - val_accuracy: 0.9334\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1845 - accuracy: 0.9320 - val_loss: 0.1656 - val_accuracy: 0.9315\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1822 - accuracy: 0.9310 - val_loss: 0.2691 - val_accuracy: 0.8878\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1852 - accuracy: 0.9210 - val_loss: 0.1599 - val_accuracy: 0.9342\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1751 - accuracy: 0.9390 - val_loss: 0.1748 - val_accuracy: 0.9377\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1865 - accuracy: 0.9250 - val_loss: 0.1631 - val_accuracy: 0.9355\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.1486 - accuracy: 0.9470 - val_loss: 0.2927 - val_accuracy: 0.8660\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1682 - accuracy: 0.9320 - val_loss: 0.1732 - val_accuracy: 0.9277\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 2s 67ms/step - loss: 0.1986 - accuracy: 0.9190 - val_loss: 0.1598 - val_accuracy: 0.9382\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 3s 85ms/step - loss: 0.1497 - accuracy: 0.9450 - val_loss: 0.1622 - val_accuracy: 0.9280\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.1501 - accuracy: 0.9470 - val_loss: 0.2290 - val_accuracy: 0.9132\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.1535 - accuracy: 0.9440 - val_loss: 0.3621 - val_accuracy: 0.8417\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1819 - accuracy: 0.9240 - val_loss: 0.3136 - val_accuracy: 0.8503\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1751 - accuracy: 0.9340 - val_loss: 0.2855 - val_accuracy: 0.8727\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.2157 - accuracy: 0.9220 - val_loss: 0.1469 - val_accuracy: 0.9447\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1398 - accuracy: 0.9520 - val_loss: 0.1478 - val_accuracy: 0.9420\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.1268 - accuracy: 0.9500 - val_loss: 0.1450 - val_accuracy: 0.9444\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1386 - accuracy: 0.9550 - val_loss: 0.1868 - val_accuracy: 0.9229\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1668 - accuracy: 0.9340 - val_loss: 0.1356 - val_accuracy: 0.9515\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1247 - accuracy: 0.9560 - val_loss: 0.2056 - val_accuracy: 0.9188\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1435 - accuracy: 0.9460 - val_loss: 0.2284 - val_accuracy: 0.9175\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1647 - accuracy: 0.9340 - val_loss: 0.1611 - val_accuracy: 0.9334\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1124 - accuracy: 0.9580 - val_loss: 0.1319 - val_accuracy: 0.9496\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.1099 - accuracy: 0.9610 - val_loss: 0.1727 - val_accuracy: 0.9401\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1016 - accuracy: 0.9610 - val_loss: 0.1370 - val_accuracy: 0.9466\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1382 - accuracy: 0.9480 - val_loss: 0.1276 - val_accuracy: 0.9512\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0858 - accuracy: 0.9660 - val_loss: 0.1425 - val_accuracy: 0.9520\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0949 - accuracy: 0.9690 - val_loss: 0.1413 - val_accuracy: 0.9517\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0956 - accuracy: 0.9650 - val_loss: 0.1829 - val_accuracy: 0.9328\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1387 - accuracy: 0.9530 - val_loss: 0.1624 - val_accuracy: 0.9302\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0960 - accuracy: 0.9640 - val_loss: 0.1252 - val_accuracy: 0.9506\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0860 - accuracy: 0.9680 - val_loss: 0.1957 - val_accuracy: 0.9275\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.1011 - accuracy: 0.9610 - val_loss: 0.1813 - val_accuracy: 0.9396\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.1079 - accuracy: 0.9690 - val_loss: 0.1352 - val_accuracy: 0.9509\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0816 - accuracy: 0.9740 - val_loss: 0.1566 - val_accuracy: 0.9428\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0793 - accuracy: 0.9680 - val_loss: 0.1301 - val_accuracy: 0.9536\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0728 - accuracy: 0.9750 - val_loss: 0.1407 - val_accuracy: 0.9512\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0779 - accuracy: 0.9740 - val_loss: 0.1820 - val_accuracy: 0.9417\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.1067 - accuracy: 0.9660 - val_loss: 0.2035 - val_accuracy: 0.9275\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0892 - accuracy: 0.9710 - val_loss: 0.1261 - val_accuracy: 0.9544\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0793 - accuracy: 0.9700 - val_loss: 0.1498 - val_accuracy: 0.9488\n"
     ]
    }
   ],
   "source": [
    "# Creazione del callback EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello\n",
    "history = cnn_inception_extended.fit(x_train_images, x_train_labels, epochs=300,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_valid_images, x_valid_labels),\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "992e0ed7-3145-49be-b443-d207088cbd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1561 - accuracy: 0.9447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15605896711349487, 0.944656491279602]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_inception_extended.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c79fd-4c7b-4a7e-a16d-8dd665f183d3",
   "metadata": {},
   "source": [
    "Best: 0.9695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece2ed8-e67d-4f0b-b288-ee98e4f29f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione della struttura del modello\n",
    "plot_model(cnn_inception_extended, to_file='model_structure.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Mostra l'immagine della struttura del modello\n",
    "img = plt.imread('model_structure.png')\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adb0b47-98bf-4ac6-90bd-c1bfb8497635",
   "metadata": {},
   "source": [
    "## Data Augmentation sul miglior modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3ac1288-1449-480c-9f4b-8d4acfdb1042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "31/31 [==============================] - 4s 69ms/step - loss: 0.5975 - accuracy: 0.7293 - val_loss: 0.6042 - val_accuracy: 0.7449\n",
      "Epoch 2/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.5889 - accuracy: 0.7293 - val_loss: 0.6072 - val_accuracy: 0.7449\n",
      "Epoch 3/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.5953 - accuracy: 0.7314 - val_loss: 0.5698 - val_accuracy: 0.7449\n",
      "Epoch 4/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.5894 - accuracy: 0.7324 - val_loss: 0.5726 - val_accuracy: 0.7449\n",
      "Epoch 5/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.5844 - accuracy: 0.7293 - val_loss: 0.5452 - val_accuracy: 0.7449\n",
      "Epoch 6/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.5446 - accuracy: 0.7314 - val_loss: 0.4651 - val_accuracy: 0.7754\n",
      "Epoch 7/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.5184 - accuracy: 0.7490 - val_loss: 0.4240 - val_accuracy: 0.8031\n",
      "Epoch 8/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.4637 - accuracy: 0.7800 - val_loss: 0.6971 - val_accuracy: 0.4725\n",
      "Epoch 9/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.4507 - accuracy: 0.7872 - val_loss: 0.3337 - val_accuracy: 0.8492\n",
      "Epoch 10/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.3994 - accuracy: 0.8089 - val_loss: 0.5390 - val_accuracy: 0.6667\n",
      "Epoch 11/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.4279 - accuracy: 0.7986 - val_loss: 0.3264 - val_accuracy: 0.8563\n",
      "Epoch 12/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.3559 - accuracy: 0.8512 - val_loss: 0.3237 - val_accuracy: 0.8587\n",
      "Epoch 13/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.4151 - accuracy: 0.8068 - val_loss: 0.3470 - val_accuracy: 0.8541\n",
      "Epoch 14/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.3778 - accuracy: 0.8481 - val_loss: 0.3701 - val_accuracy: 0.8433\n",
      "Epoch 15/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.3448 - accuracy: 0.8481 - val_loss: 0.3545 - val_accuracy: 0.8045\n",
      "Epoch 16/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.3199 - accuracy: 0.8709 - val_loss: 0.2690 - val_accuracy: 0.8749\n",
      "Epoch 17/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.3099 - accuracy: 0.8760 - val_loss: 0.3447 - val_accuracy: 0.8209\n",
      "Epoch 18/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2876 - accuracy: 0.8936 - val_loss: 0.5024 - val_accuracy: 0.7400\n",
      "Epoch 19/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.3045 - accuracy: 0.8874 - val_loss: 0.2448 - val_accuracy: 0.8921\n",
      "Epoch 20/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2553 - accuracy: 0.9081 - val_loss: 0.2158 - val_accuracy: 0.9080\n",
      "Epoch 21/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2530 - accuracy: 0.9091 - val_loss: 0.2014 - val_accuracy: 0.9134\n",
      "Epoch 22/300\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.2448 - accuracy: 0.9019 - val_loss: 0.2277 - val_accuracy: 0.9013\n",
      "Epoch 23/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2231 - accuracy: 0.9153 - val_loss: 0.2065 - val_accuracy: 0.9150\n",
      "Epoch 24/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.2364 - accuracy: 0.9081 - val_loss: 0.1879 - val_accuracy: 0.9248\n",
      "Epoch 25/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2391 - accuracy: 0.9039 - val_loss: 0.2208 - val_accuracy: 0.9140\n",
      "Epoch 26/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2430 - accuracy: 0.8998 - val_loss: 0.3329 - val_accuracy: 0.8487\n",
      "Epoch 27/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1954 - accuracy: 0.9225 - val_loss: 0.1781 - val_accuracy: 0.9315\n",
      "Epoch 28/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1897 - accuracy: 0.9287 - val_loss: 0.3718 - val_accuracy: 0.8376\n",
      "Epoch 29/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.2039 - accuracy: 0.9225 - val_loss: 0.1857 - val_accuracy: 0.9261\n",
      "Epoch 30/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.1837 - accuracy: 0.9339 - val_loss: 0.2369 - val_accuracy: 0.8929\n",
      "Epoch 31/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.2112 - accuracy: 0.9184 - val_loss: 0.3245 - val_accuracy: 0.8560\n",
      "Epoch 32/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1830 - accuracy: 0.9277 - val_loss: 0.1412 - val_accuracy: 0.9415\n",
      "Epoch 33/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2269 - accuracy: 0.9070 - val_loss: 0.2671 - val_accuracy: 0.8916\n",
      "Epoch 34/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2420 - accuracy: 0.9019 - val_loss: 0.2023 - val_accuracy: 0.9150\n",
      "Epoch 35/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.1927 - accuracy: 0.9215 - val_loss: 0.1489 - val_accuracy: 0.9463\n",
      "Epoch 36/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2021 - accuracy: 0.9215 - val_loss: 0.1717 - val_accuracy: 0.9331\n",
      "Epoch 37/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1768 - accuracy: 0.9380 - val_loss: 0.1541 - val_accuracy: 0.9391\n",
      "Epoch 38/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1984 - accuracy: 0.9236 - val_loss: 0.1487 - val_accuracy: 0.9426\n",
      "Epoch 39/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1762 - accuracy: 0.9339 - val_loss: 0.1373 - val_accuracy: 0.9439\n",
      "Epoch 40/300\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.1713 - accuracy: 0.9298 - val_loss: 0.1286 - val_accuracy: 0.9490\n",
      "Epoch 41/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1495 - accuracy: 0.9432 - val_loss: 0.1352 - val_accuracy: 0.9450\n",
      "Epoch 42/300\n",
      "31/31 [==============================] - 2s 62ms/step - loss: 0.1483 - accuracy: 0.9421 - val_loss: 0.2164 - val_accuracy: 0.9105\n",
      "Epoch 43/300\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.1668 - accuracy: 0.9339 - val_loss: 0.1558 - val_accuracy: 0.9369\n",
      "Epoch 44/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1833 - accuracy: 0.9318 - val_loss: 0.1463 - val_accuracy: 0.9431\n",
      "Epoch 45/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1682 - accuracy: 0.9360 - val_loss: 0.1433 - val_accuracy: 0.9442\n",
      "Epoch 46/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1711 - accuracy: 0.9390 - val_loss: 0.1999 - val_accuracy: 0.9094\n",
      "Epoch 47/300\n",
      "31/31 [==============================] - 2s 70ms/step - loss: 0.1523 - accuracy: 0.9401 - val_loss: 0.1523 - val_accuracy: 0.9358\n",
      "Epoch 48/300\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.1507 - accuracy: 0.9452 - val_loss: 0.1602 - val_accuracy: 0.9331\n",
      "Epoch 49/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.1445 - accuracy: 0.9380 - val_loss: 0.3045 - val_accuracy: 0.9134\n",
      "Epoch 50/300\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.2235 - accuracy: 0.9163 - val_loss: 0.2035 - val_accuracy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "# Reshape dei dati di input\n",
    "x_reshaped_train = x_train_images.reshape(-1, 28, 28, 1)\n",
    "x_reshaped_valid = x_valid_images.reshape(-1, 28, 28, 1)\n",
    "test_reshaped = test_images.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Inizializzazione dei pesi\n",
    "init = HeNormal(seed=42)\n",
    "\n",
    "# Definizione della CNN\n",
    "input_shape = (28, 28, 1)\n",
    "cnn_inception_extended_da = create_inception_cnn(input_shape)\n",
    "\n",
    "# Compilazione del modello\n",
    "cnn_inception_extended_da.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Configurazione della Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=0.05,  # no rotazione\n",
    "    width_shift_range=0.05,  # Spostamento casuale orizzontale del 10% della larghezza\n",
    "    height_shift_range=0.05,  # Spostamento casuale verticale del 10% dell'altezza\n",
    "    zoom_range=0.05,  # Zoom casuale del 10%\n",
    "    horizontal_flip=False,  # Ribaltamento casuale orizzontale\n",
    "    vertical_flip=False  # Senza ribaltamento casuale verticale\n",
    ")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Addestramento del modello con Data Augmentation\n",
    "history_cnn_inception_extended_da = cnn_inception_extended_da.fit(\n",
    "    datagen.flow(x_reshaped_train, x_train_labels, batch_size=32),\n",
    "    steps_per_epoch=len(x_reshaped_train) // 32,\n",
    "    epochs=300,\n",
    "    validation_data=(x_reshaped_valid, x_valid_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57fb95f7-37c0-4c6a-a4b2-3509fbf06c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1086 - accuracy: 0.9599\n",
      "Test Loss: 0.10857484489679337\n",
      "Test Accuracy: 0.9599236845970154\n"
     ]
    }
   ],
   "source": [
    "# Valutazione del modello sui dati di test\n",
    "test_loss, test_accuracy = cnn_inception_extended_da.evaluate(test_reshaped, test_labels)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947672dc-be0a-478d-9183-ad5ae020b523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
